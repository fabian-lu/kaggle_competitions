{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Multi-Model Ensemble with Custom Loss Functions\n",
    "\n",
    "Strategy:\n",
    "1. Custom asymmetric loss for all three algorithms\n",
    "2. Advanced feature engineering\n",
    "3. Validation-based sector analysis\n",
    "4. Temporal smoothing post-processing\n",
    "5. Optimized ensemble weights"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:19:49.239627Z",
     "start_time": "2025-10-05T17:19:49.237880Z"
    }
   },
   "source": "TEST_MODE = True",
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:19:49.297969Z",
     "start_time": "2025-10-05T17:19:49.293584Z"
    }
   },
   "source": "import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.cluster import KMeans\nimport optuna  # Bayesian optimization\nimport warnings\nwarnings.filterwarnings('ignore')\noptuna.logging.set_verbosity(optuna.logging.WARNING)  # Reduce optuna output\n\nprint(f\"Running in {'TEST' if TEST_MODE else 'PRODUCTION'} mode\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in TEST mode\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Metric + Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:19:49.364302Z",
     "start_time": "2025-10-05T17:19:49.354559Z"
    }
   },
   "source": "def competition_metric(y_true, y_pred, eps=1e-10, verbose=True):\n    \"\"\"\n    Two-stage metric from competition rules\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    \n    ape = np.abs(y_true - y_pred) / np.maximum(y_true, eps)\n    \n    extreme_errors = (ape > 1.0).sum() / len(ape)\n    if verbose:\n        print(f\"  APE > 100%: {extreme_errors*100:.1f}%\")\n    \n    if extreme_errors > 0.3:\n        if verbose:\n            print(f\"  FAILED Stage 1\")\n        return 0.0\n    \n    valid_mask = ape <= 1.0\n    if valid_mask.sum() == 0:\n        return 0.0\n    \n    mape = ape[valid_mask].mean()\n    fraction_valid = valid_mask.sum() / len(ape)\n    scaled_mape = mape / fraction_valid\n    score = max(0, 1 - scaled_mape)\n    \n    if verbose:\n        print(f\"  Valid samples: {fraction_valid*100:.1f}%\")\n        print(f\"  MAPE (valid): {mape:.4f}\")\n        print(f\"  Scaled MAPE: {scaled_mape:.4f}\")\n    \n    return score\n\n\n# === ADAPTIVE CATBOOST LOSS ===\nclass CatBoostAdaptiveLoss(object):\n    \"\"\"Adaptive asymmetric loss - stronger penalty for larger over-predictions\"\"\"\n    def calc_ders_range(self, approxes, targets, weights):\n        assert len(approxes) == len(targets)\n        result = []\n        for i in range(len(targets)):\n            diff = targets[i] - approxes[i]\n            \n            # Calculate penalty based on magnitude of over-prediction\n            if approxes[i] > targets[i]:\n                # Over-predicting\n                magnitude = (approxes[i] - targets[i]) / (targets[i] + 1e-10)\n                if magnitude > 0.5:  # Extreme over-prediction (>50%)\n                    penalty = 10.0\n                elif magnitude > 0.2:  # Moderate over-prediction (20-50%)\n                    penalty = 5.0\n                else:  # Small over-prediction (<20%)\n                    penalty = 3.0\n                der1 = penalty * np.sign(diff)\n            else:\n                # Under-predicting - small penalty\n                der1 = 1.0 * np.sign(diff)\n            \n            der2 = 0\n            result.append((der1, der2))\n        return result\n\n\nclass CatBoostCompetitionMetric:\n    def is_max_optimal(self):\n        return True\n    \n    def evaluate(self, approxes, target, weight):\n        assert len(approxes) == 1\n        score = competition_metric(target, approxes[0], verbose=False)\n        return score, 1\n    \n    def get_final_error(self, error, weight):\n        return error\n\n\n# === ADAPTIVE XGBOOST LOSS ===\ndef xgb_adaptive_loss(y_pred, dtrain):\n    \"\"\"\n    Adaptive asymmetric loss for XGBoost\n    Stronger penalty for larger over-predictions\n    \"\"\"\n    y_true = dtrain.get_label()\n    residual = y_pred - y_true\n    \n    # Adaptive penalty based on magnitude\n    magnitude = np.abs(residual) / (y_true + 1e-10)\n    over_pred_mask = y_pred > y_true\n    \n    # Initialize with base gradient\n    grad = np.ones_like(y_true) * residual\n    \n    # Apply adaptive penalties for over-predictions\n    extreme_mask = over_pred_mask & (magnitude > 0.5)\n    moderate_mask = over_pred_mask & (magnitude > 0.2) & (magnitude <= 0.5)\n    small_mask = over_pred_mask & (magnitude <= 0.2)\n    \n    grad[extreme_mask] = 10.0 * residual[extreme_mask]\n    grad[moderate_mask] = 5.0 * residual[moderate_mask]\n    grad[small_mask] = 3.0 * residual[small_mask]\n    \n    # Hessian (constant for simplicity)\n    hess = np.ones_like(y_true)\n    \n    return grad, hess\n\n\n# === ADAPTIVE LIGHTGBM LOSS ===\ndef lgb_adaptive_loss(y_true, y_pred):\n    \"\"\"\n    Adaptive asymmetric loss for LightGBM\n    \"\"\"\n    residual = y_pred - y_true\n    \n    # Adaptive penalty\n    magnitude = np.abs(residual) / (y_true + 1e-10)\n    over_pred_mask = y_pred > y_true\n    \n    grad = np.ones_like(y_true) * residual\n    \n    extreme_mask = over_pred_mask & (magnitude > 0.5)\n    moderate_mask = over_pred_mask & (magnitude > 0.2) & (magnitude <= 0.5)\n    small_mask = over_pred_mask & (magnitude <= 0.2)\n    \n    grad[extreme_mask] = 10.0 * residual[extreme_mask]\n    grad[moderate_mask] = 5.0 * residual[moderate_mask]\n    grad[small_mask] = 3.0 * residual[small_mask]\n    \n    hess = np.ones_like(y_true)\n    \n    return grad, hess\n\n\nprint(\"Adaptive loss functions defined\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive loss functions defined\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:19:49.447976Z",
     "start_time": "2025-10-05T17:19:49.410432Z"
    }
   },
   "source": [
    "file_path = \"./data/\"\n",
    "\n",
    "new_house = pd.read_csv(file_path + 'train/new_house_transactions.csv')\n",
    "new_house_nearby = pd.read_csv(file_path + 'train/new_house_transactions_nearby_sectors.csv')\n",
    "pre_owned = pd.read_csv(file_path + 'train/pre_owned_house_transactions.csv')\n",
    "pre_owned_nearby = pd.read_csv(file_path + 'train/pre_owned_house_transactions_nearby_sectors.csv')\n",
    "land_trans = pd.read_csv(file_path + 'train/land_transactions.csv')\n",
    "land_trans_nearby = pd.read_csv(file_path + 'train/land_transactions_nearby_sectors.csv')\n",
    "sector_poi = pd.read_csv(file_path + 'train/sector_POI.csv')\n",
    "city_indexes = pd.read_csv(file_path + 'train/city_indexes.csv')\n",
    "search_index = pd.read_csv(file_path + 'train/city_search_index.csv')\n",
    "test = pd.read_csv(file_path + 'test.csv')\n",
    "test[['month', 'sector']] = test['id'].str.split('_', n=1, expand=True)\n",
    "\n",
    "print(f\"Training samples: {len(new_house)}\")\n",
    "print(f\"Test samples: {len(test)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5433\n",
      "Test samples: 1152\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:19:49.475632Z",
     "start_time": "2025-10-05T17:19:49.466190Z"
    }
   },
   "source": "def create_base_features(df, target='amount_new_house_transactions', sector_clusters=None):\n    \"\"\"\n    Enhanced feature engineering - NO TARGET-DERIVED FEATURES AT ALL\n    \"\"\"\n    data = df.copy()\n    \n    # Merge all datasets EXCEPT new_house (all new_house columns are derived from transactions we're predicting)\n    data = data.merge(new_house_nearby, on=['month', 'sector'], how='left')\n    data = data.merge(pre_owned, on=['month', 'sector'], how='left')\n    data = data.merge(pre_owned_nearby, on=['month', 'sector'], how='left')\n    data = data.merge(land_trans, on=['month', 'sector'], how='left')\n    data = data.merge(land_trans_nearby, on=['month', 'sector'], how='left')\n    data = data.merge(sector_poi, on='sector', how='left')\n    \n    # DO NOT MERGE new_house AT ALL - every column in it is derived from the target\n    # - price_new_house_transactions: calculated FROM actual transactions\n    # - num/area_new_house_transactions: the transaction data we're predicting\n    # - num/area_new_house_available_for_sale: inventory derived from transaction outcomes\n    # - period_new_house_sell_through: calculated FROM transaction velocity\n    \n    # DateTime features\n    data['month_date'] = pd.to_datetime(data['month'], format='%Y-%b' if 'id' not in data.columns else '%Y %b')\n    data['year'] = data['month_date'].dt.year\n    data['month_num'] = data['month_date'].dt.month\n    data['quarter'] = data['month_date'].dt.quarter\n    \n    # City indexes\n    data = data.merge(city_indexes, left_on='year', right_on='city_indicator_data_year', how='left')\n    \n    # Search aggregates\n    search_agg = search_index.groupby('month').agg({\n        'search_volume': ['sum', 'mean', 'max', 'std']\n    }).reset_index()\n    search_agg.columns = ['month', 'search_volume_sum', 'search_volume_mean', \n                          'search_volume_max', 'search_volume_std']\n    data = data.merge(search_agg, on='month', how='left')\n    \n    # Cyclic features\n    data['month_sin'] = np.sin(2 * np.pi * data['month_num'] / 12)\n    data['month_cos'] = np.cos(2 * np.pi * data['month_num'] / 12)\n    data['quarter_sin'] = np.sin(2 * np.pi * data['quarter'] / 4)\n    data['quarter_cos'] = np.cos(2 * np.pi * data['quarter'] / 4)\n    \n    # === RATIO FEATURES (all valid, no target leakage) ===\n    \n    # Pre-owned market dynamics (separate market from new houses - valid)\n    data['nearby_to_local_preowned_ratio'] = (\n        data['amount_pre_owned_house_transactions_nearby_sectors'] / \n        (data['amount_pre_owned_house_transactions'] + 1)\n    )\n    \n    # Land transaction features\n    data['land_price_per_area'] = (\n        data['transaction_amount'] / \n        (data['construction_area'] + 1)\n    )\n    data['land_planned_vs_construction'] = (\n        data['planned_building_area'] / \n        (data['construction_area'] + 1)\n    )\n    \n    # Population-based features\n    data['price_per_resident'] = (\n        data['surrounding_housing_average_price'] / \n        (data['resident_population'] + 1)\n    )\n    data['shops_per_population'] = (\n        data['number_of_shops'] / \n        (data['resident_population'] + 1)\n    )\n    data['office_to_resident_ratio'] = (\n        data['office_population'] / \n        (data['resident_population'] + 1)\n    )\n    \n    # Sector cluster features (if provided)\n    if sector_clusters is not None:\n        data = data.merge(sector_clusters, on='sector', how='left')\n    \n    return data\n\n\ndef create_lag_features(data, target='amount_new_house_transactions'):\n    \"\"\"\n    Enhanced lag features - these are VALID (shifted target, no leakage)\n    \"\"\"\n    data = data.sort_values(['sector', 'month_date']).copy()\n    \n    # Target encoding (shifted)\n    data['sector_target_encoded'] = data.groupby('sector')[target].transform(\n        lambda x: x.shift(1).expanding().mean()\n    )\n    \n    # Expanded lags\n    for lag in [1, 2, 3, 4, 5, 6, 7, 9, 12, 18, 24]:\n        data[f'amount_lag_{lag}'] = data.groupby('sector')[target].shift(lag)\n    \n    # EMAs\n    for span in [3, 6, 12]:\n        data[f'amount_ewm_{span}'] = data.groupby('sector')[target].transform(\n            lambda x: x.shift(1).ewm(span=span, adjust=False).mean()\n        )\n    \n    # Rolling stats\n    for window in [3, 6, 12]:\n        data[f'amount_rolling_mean_{window}'] = data.groupby('sector')[target].transform(\n            lambda x: x.shift(1).rolling(window, min_periods=1).mean()\n        )\n        data[f'amount_rolling_std_{window}'] = data.groupby('sector')[target].transform(\n            lambda x: x.shift(1).rolling(window, min_periods=1).std()\n        )\n    \n    # Momentum/acceleration\n    data['mom_change_1m'] = (\n        (data.groupby('sector')[target].shift(1) - data.groupby('sector')[target].shift(2)) /\n        (data.groupby('sector')[target].shift(2) + 1)\n    )\n    data['mom_change_3m'] = (\n        (data.groupby('sector')[target].shift(1) - data.groupby('sector')[target].shift(4)) /\n        (data.groupby('sector')[target].shift(4) + 1)\n    )\n    data['amount_lag_12_growth'] = (\n        (data.groupby('sector')[target].shift(1) - data.groupby('sector')[target].shift(13)) /\n        (data.groupby('sector')[target].shift(13) + 1)\n    )\n    data['qoq_growth'] = (\n        (data.groupby('sector')[target].shift(1) - data.groupby('sector')[target].shift(4)) /\n        (data.groupby('sector')[target].shift(4) + 1)\n    )\n    data['trend_3m'] = (\n        (data.groupby('sector')[target].shift(1) - data.groupby('sector')[target].shift(4)) /\n        (data.groupby('sector')[target].shift(4) + 1)\n    )\n    data['trend_6m'] = (\n        (data.groupby('sector')[target].shift(1) - data.groupby('sector')[target].shift(7)) /\n        (data.groupby('sector')[target].shift(7) + 1)\n    )\n    data['acceleration_3m'] = data['trend_3m'] - data.groupby('sector')['trend_3m'].shift(3)\n    data['volatility_6m'] = data.groupby('sector')[target].transform(\n        lambda x: x.shift(1).rolling(6, min_periods=1).std() / (x.shift(1).rolling(6, min_periods=1).mean() + 1)\n    )\n    \n    return data\n\n\ndef add_sector_stats(data, train_data, target='amount_new_house_transactions'):\n    \"\"\"\n    Sector statistics - VALID (from training data only)\n    \"\"\"\n    sector_stats = train_data.groupby('sector')[target].agg(['mean', 'std', 'min', 'max', 'median']).reset_index()\n    sector_stats.columns = ['sector', 'sector_mean', 'sector_std', 'sector_min', 'sector_max', 'sector_median']\n    sector_stats['sector_cv'] = sector_stats['sector_std'] / (sector_stats['sector_mean'] + 1)\n    data = data.merge(sector_stats, on='sector', how='left')\n    return data\n\n\nprint(\"Feature engineering functions defined - ZERO TARGET LEAKAGE!\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering functions defined - ZERO TARGET LEAKAGE!\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Build Complete Grid + Create Sector Clusters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:19:49.544950Z",
     "start_time": "2025-10-05T17:19:49.526202Z"
    }
   },
   "source": [
    "target = 'amount_new_house_transactions'\n",
    "\n",
    "# Build dense grid\n",
    "months = sorted(set(new_house['month']) | set(pre_owned['month']) | set(land_trans['month']))\n",
    "sectors = sorted(set(new_house['sector']) | set(pre_owned['sector']) | set(land_trans['sector']))\n",
    "train_grid = pd.DataFrame([(m, s) for m in months for s in sectors], columns=['month', 'sector'])\n",
    "\n",
    "# Attach target (zeros when missing)\n",
    "train_grid = train_grid.merge(new_house[['month', 'sector', target]], on=['month', 'sector'], how='left')\n",
    "train_grid[target] = train_grid[target].fillna(0)\n",
    "\n",
    "print(f\"Grid size: {len(train_grid)} (was {len(new_house)} observed)\")\n",
    "print(f\"Zero-filled entries: {(train_grid[target] == 0).sum()}\")\n",
    "\n",
    "# Create sector clusters based on POI features\n",
    "poi_features = ['resident_population', 'office_population', 'number_of_shops', \n",
    "                'transportation_station', 'education', 'surrounding_housing_average_price']\n",
    "poi_data = sector_poi[['sector'] + poi_features].fillna(0)\n",
    "\n",
    "# K-Means clustering\n",
    "n_clusters = 5 if not TEST_MODE else 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "poi_data['sector_cluster'] = kmeans.fit_predict(poi_data[poi_features])\n",
    "sector_clusters = poi_data[['sector', 'sector_cluster']]\n",
    "\n",
    "print(f\"Created {n_clusters} sector clusters\")\n",
    "print(sector_clusters.groupby('sector_cluster').size())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 6432 (was 5433 observed)\n",
      "Zero-filled entries: 999\n",
      "Created 3 sector clusters\n",
      "sector_cluster\n",
      "0    12\n",
      "1     1\n",
      "2    73\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Feature Engineering + Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:19:49.966542Z",
     "start_time": "2025-10-05T17:19:49.577989Z"
    }
   },
   "source": "# Build features on grid\nall_data = create_base_features(train_grid, target=target, sector_clusters=sector_clusters)\n\n# ONLY USE RECENT 24 MONTHS for training (like leader uses 18 months)\n# This captures current market regime, not ancient 2019-2020 COVID data\nrecent_months = 24\ncutoff_date = all_data['month_date'].max() - pd.DateOffset(months=recent_months)\nall_data = all_data[all_data['month_date'] >= cutoff_date].copy()\n\nprint(f\"Using only recent {recent_months} months: {all_data['month_date'].min()} to {all_data['month_date'].max()}\")\n\n# Time-based split (last 6 months for validation)\nval_months = 6\nval_cutoff = all_data['month_date'].max() - pd.DateOffset(months=val_months)\n\ntrain_data = all_data[all_data['month_date'] < val_cutoff].copy()\nval_data = all_data[all_data['month_date'] >= val_cutoff].copy()\n\nprint(f\"Train: {len(train_data)} ({train_data['month_date'].min()} to {train_data['month_date'].max()})\")\nprint(f\"Val: {len(val_data)} ({val_data['month_date'].min()} to {val_data['month_date'].max()})\")\n\n# Lag features\ntrain_data = create_lag_features(train_data, target=target)\n\ncombined = pd.concat([train_data, val_data]).sort_values(['sector', 'month_date'])\ncombined = create_lag_features(combined, target=target)\nval_data = combined[combined['month_date'] >= val_cutoff].copy()\n\n# Sector stats\ntrain_data = add_sector_stats(train_data, train_data, target=target)\nval_data = add_sector_stats(val_data, train_data, target=target)\n\n# Define features - REMOVED all new_house inventory columns\nexclude_cols = [\n    'month', 'sector', 'month_date', target, 'city_indicator_data_year',\n]\nfeature_cols = [col for col in train_data.columns\n                if col not in exclude_cols and train_data[col].dtype in ['int64', 'float64', 'int8', 'float32']]\n\nX_train = train_data[feature_cols].fillna(0)\ny_train = train_data[target]\nX_val = val_data[feature_cols].fillna(0)\ny_val = val_data[target]\n\nprint(f\"Features: {len(feature_cols)}\")\nprint(f\"X_train: {X_train.shape}, X_val: {X_val.shape}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using only recent 24 months: 2022-07-01 00:00:00 to 2024-07-01 00:00:00\n",
      "Train: 2304 (2022-07-01 00:00:00 to 2023-12-01 00:00:00)\n",
      "Val: 672 (2024-01-01 00:00:00 to 2024-07-01 00:00:00)\n",
      "Features: 289\n",
      "X_train: (2304, 289), X_val: (672, 289)\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Hyperparameter Tuning (with Custom Loss)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:21:55.326379Z",
     "start_time": "2025-10-05T17:19:49.971616Z"
    }
   },
   "source": [
    "# Choose optimization method - USE OPTUNA IN BOTH MODES\n",
    "USE_OPTUNA = True  # Always use Bayesian optimization\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\"[TEST MODE] Using Optuna with reduced trials for quick testing\")\n",
    "    n_trials_xgb = 10  # Quick search\n",
    "    n_trials_lgb = 10\n",
    "    n_trials_cat = 15  # More for CatBoost since it's most important\n",
    "else:\n",
    "    print(\"[PRODUCTION MODE] Using Optuna Bayesian optimization\")\n",
    "    n_trials_xgb = 200  # Search 100 combinations intelligently\n",
    "    n_trials_lgb = 200\n",
    "    n_trials_cat = 200  # More for CatBoost since it's most important\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "# ==============================================================================\n",
    "# OPTUNA OPTIMIZATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    \"\"\"Optuna objective for XGBoost\"\"\"\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 10),\n",
    "        'eta': trial.suggest_float('eta', 0.005, 0.03, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.95),\n",
    "        'gamma': trial.suggest_float('gamma', 0.01, 0.3),\n",
    "        'alpha': trial.suggest_float('alpha', 0.01, 0.3),\n",
    "        'lambda': trial.suggest_float('lambda', 0.5, 2.0),\n",
    "        'seed': 42\n",
    "    }\n",
    "    n_est = trial.suggest_int('n_estimators', 1000, 4000, step=500)\n",
    "    \n",
    "    model = xgb.train(\n",
    "        params, dtrain,\n",
    "        num_boost_round=n_est,\n",
    "        obj=xgb_adaptive_loss,\n",
    "        evals=[(dval, 'val')],\n",
    "        early_stopping_rounds=200 if not TEST_MODE else 50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    pred = np.maximum(model.predict(dval), 0)\n",
    "    score = competition_metric(y_val, pred, verbose=False)\n",
    "    \n",
    "    # Store best iteration for later use\n",
    "    trial.set_user_attr('best_iteration', model.best_iteration)\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def objective_lgb(trial):\n",
    "    \"\"\"Optuna objective for LightGBM\"\"\"\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.03, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 31, 255),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 0.95),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.95),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 0.3),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.5, 2.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1000, 4000, step=500),\n",
    "        'objective': lgb_adaptive_loss,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[lgb.early_stopping(200 if not TEST_MODE else 50, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    pred = np.maximum(model.predict(X_val), 0)\n",
    "    score = competition_metric(y_val, pred, verbose=False)\n",
    "    \n",
    "    trial.set_user_attr('best_iteration', model.best_iteration_)\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def objective_cat(trial):\n",
    "    \"\"\"Optuna objective for CatBoost - MOST IMPORTANT\"\"\"\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 5000, 25000, step=2500) if not TEST_MODE else 500,\n",
    "        'depth': trial.suggest_int('depth', 5, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.04, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.1, 1.0),\n",
    "        'random_seed': 42,\n",
    "        'loss_function': CatBoostAdaptiveLoss(),\n",
    "        'eval_metric': CatBoostCompetitionMetric(),\n",
    "        'early_stopping_rounds': 500 if not TEST_MODE else 100,\n",
    "        'verbose': False\n",
    "    }\n",
    "    \n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
    "    \n",
    "    pred = np.maximum(model.predict(X_val), 0)\n",
    "    score = competition_metric(y_val, pred, verbose=False)\n",
    "    \n",
    "    trial.set_user_attr('best_iteration', model.get_best_iteration())\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# HYPERPARAMETER OPTIMIZATION WITH OPTUNA\n",
    "# ==============================================================================\n",
    "\n",
    "# === XGBOOST OPTIMIZATION ===\n",
    "print(f\"\\nOptimizing XGBoost with Optuna ({n_trials_xgb} trials)...\")\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name='xgb_optimization')\n",
    "study_xgb.optimize(objective_xgb, n_trials=n_trials_xgb, show_progress_bar=True)\n",
    "\n",
    "best_xgb_params = study_xgb.best_params\n",
    "best_xgb_params['n_estimators'] = study_xgb.best_trial.user_attrs['best_iteration']\n",
    "best_xgb_score = study_xgb.best_value\n",
    "\n",
    "print(f\"Best XGBoost params: {best_xgb_params}\")\n",
    "print(f\"Best XGBoost score: {best_xgb_score:.4f}\")\n",
    "\n",
    "# === LIGHTGBM OPTIMIZATION ===\n",
    "print(f\"\\nOptimizing LightGBM with Optuna ({n_trials_lgb} trials)...\")\n",
    "study_lgb = optuna.create_study(direction='maximize', study_name='lgb_optimization')\n",
    "study_lgb.optimize(objective_lgb, n_trials=n_trials_lgb, show_progress_bar=True)\n",
    "\n",
    "best_lgb_params = study_lgb.best_params\n",
    "best_lgb_params['n_estimators'] = study_lgb.best_trial.user_attrs['best_iteration']\n",
    "best_lgb_score = study_lgb.best_value\n",
    "\n",
    "print(f\"Best LightGBM params: {best_lgb_params}\")\n",
    "print(f\"Best LightGBM score: {best_lgb_score:.4f}\")\n",
    "\n",
    "# === CATBOOST OPTIMIZATION (MOST IMPORTANT!) ===\n",
    "print(f\"\\nOptimizing CatBoost with Optuna ({n_trials_cat} trials) - THIS IS KEY...\")\n",
    "study_cat = optuna.create_study(direction='maximize', study_name='cat_optimization')\n",
    "study_cat.optimize(objective_cat, n_trials=n_trials_cat, show_progress_bar=True)\n",
    "\n",
    "best_cat_params = study_cat.best_params\n",
    "best_cat_params['iterations'] = study_cat.best_trial.user_attrs['best_iteration']\n",
    "best_cat_score = study_cat.best_value\n",
    "\n",
    "print(f\"Best CatBoost params: {best_cat_params}\")\n",
    "print(f\"Best CatBoost score: {best_cat_score:.4f}\")\n",
    "\n",
    "# Rename keys to match expected format\n",
    "if 'eta' in best_xgb_params:\n",
    "    best_xgb_params['learning_rate'] = best_xgb_params.pop('eta')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPERPARAMETER SEARCH COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best XGBoost score: {best_xgb_score:.4f}\")\n",
    "print(f\"Best LightGBM score: {best_lgb_score:.4f}\")\n",
    "print(f\"Best CatBoost score: {best_cat_score:.4f}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST MODE] Using Optuna with reduced trials for quick testing\n",
      "\n",
      "Optimizing XGBoost with Optuna (10 trials)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 9. Best value: 0.505315: 100%|██████████| 10/10 [00:52<00:00,  5.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost params: {'max_depth': 5, 'eta': 0.00597986549809105, 'subsample': 0.7075688195363431, 'colsample_bytree': 0.8595248529280993, 'gamma': 0.1670166652998772, 'alpha': 0.1860811063711681, 'lambda': 1.8621618644332167, 'n_estimators': 665}\n",
      "Best XGBoost score: 0.5053\n",
      "\n",
      "Optimizing LightGBM with Optuna (10 trials)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.517638: 100%|██████████| 10/10 [00:11<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LightGBM params: {'max_depth': 10, 'learning_rate': 0.013790438408195164, 'num_leaves': 118, 'subsample': 0.8487802322757152, 'colsample_bytree': 0.8384259258595648, 'reg_alpha': 0.1538328052586716, 'reg_lambda': 0.5506559891687581, 'n_estimators': 147}\n",
      "Best LightGBM score: 0.5176\n",
      "\n",
      "Optimizing CatBoost with Optuna (15 trials) - THIS IS KEY...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.456216: 100%|██████████| 15/15 [01:00<00:00,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CatBoost params: {'depth': 5, 'learning_rate': 0.022658164793916934, 'l2_leaf_reg': 0.23438768621794598, 'iterations': 499}\n",
      "Best CatBoost score: 0.4562\n",
      "\n",
      "============================================================\n",
      "HYPERPARAMETER SEARCH COMPLETE\n",
      "============================================================\n",
      "Best XGBoost score: 0.5053\n",
      "Best LightGBM score: 0.5176\n",
      "Best CatBoost score: 0.4562\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Train Ensemble with Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:22:21.418512Z",
     "start_time": "2025-10-05T17:21:55.552805Z"
    }
   },
   "source": "seeds = [42, 123] if TEST_MODE else [42, 123, 456, 789, 2024, 1337, 9999, 777, 888, 111, 222, 333, 444, 555, 666]\n\nprint(f\"\\nTraining {len(seeds)} models per algorithm with adaptive loss...\")\n\n# XGBoost (native API with adaptive loss)\nxgb_models = []\nfor seed in seeds:\n    # Handle Optuna params (has 'eta') vs grid search params (has 'learning_rate')\n    lr = best_xgb_params.get('eta', best_xgb_params.get('learning_rate', 0.02))\n    \n    params = {\n        'max_depth': best_xgb_params['max_depth'],\n        'eta': lr,\n        'subsample': best_xgb_params.get('subsample', 0.8),\n        'colsample_bytree': best_xgb_params.get('colsample_bytree', 0.8),\n        'gamma': best_xgb_params.get('gamma', 0.1),\n        'alpha': best_xgb_params.get('alpha', 0.1),\n        'lambda': best_xgb_params.get('lambda', 1.0),\n        'seed': seed\n    }\n    model = xgb.train(\n        params, dtrain,\n        num_boost_round=best_xgb_params['n_estimators'],\n        obj=xgb_adaptive_loss,\n        verbose_eval=False\n    )\n    xgb_models.append(model)\nprint(f\"  XGBoost: {len(xgb_models)} models trained\")\n\n# LightGBM (sklearn API with adaptive loss)\nlgb_models = []\nfor seed in seeds:\n    model = lgb.LGBMRegressor(\n        n_estimators=best_lgb_params['n_estimators'],\n        max_depth=best_lgb_params['max_depth'],\n        learning_rate=best_lgb_params['learning_rate'],\n        num_leaves=best_lgb_params.get('num_leaves', 2**best_lgb_params['max_depth']),\n        subsample=best_lgb_params.get('subsample', 0.8),\n        colsample_bytree=best_lgb_params.get('colsample_bytree', 0.8),\n        reg_alpha=best_lgb_params.get('reg_alpha', 0.1),\n        reg_lambda=best_lgb_params.get('reg_lambda', 1.0),\n        objective=lgb_adaptive_loss,\n        random_state=seed,\n        n_jobs=-1,\n        verbose=-1\n    )\n    model.fit(X_train, y_train)\n    lgb_models.append(model)\nprint(f\"  LightGBM: {len(lgb_models)} models trained\")\n\n# CatBoost (with adaptive loss)\ncat_models = []\nfor seed in seeds:\n    model = CatBoostRegressor(\n        iterations=best_cat_params['iterations'],\n        depth=best_cat_params['depth'],\n        learning_rate=best_cat_params['learning_rate'],\n        l2_leaf_reg=best_cat_params.get('l2_leaf_reg', 0.3),\n        random_seed=seed,\n        loss_function=CatBoostAdaptiveLoss(),\n        eval_metric=CatBoostCompetitionMetric(),\n        metric_period=9999,\n        verbose=False\n    )\n    model.fit(X_train, y_train, eval_set=(X_val, y_val))\n    cat_models.append(model)\nprint(f\"  CatBoost: {len(cat_models)} models trained\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training 2 models per algorithm with adaptive loss...\n",
      "  XGBoost: 2 models trained\n",
      "  LightGBM: 2 models trained\n",
      "  CatBoost: 2 models trained\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Optimize Ensemble Weights"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:22:21.736555Z",
     "start_time": "2025-10-05T17:22:21.605574Z"
    }
   },
   "source": "# Get predictions (ensure non-negative)\nxgb_val_preds = np.maximum(np.mean([m.predict(dval) for m in xgb_models], axis=0), 0)\nlgb_val_preds = np.maximum(np.mean([m.predict(X_val) for m in lgb_models], axis=0), 0)\ncat_val_preds = np.maximum(np.mean([m.predict(X_val) for m in cat_models], axis=0), 0)\n\nprint(\"Optimizing ensemble weights...\")\n\ngrid = np.linspace(0, 1, 11 if TEST_MODE else 21)\nbest_score = -1\nbest_weights = None\n\nfor w_xgb in grid:\n    for w_lgb in grid:\n        w_cat = 1.0 - w_xgb - w_lgb\n        if w_cat < 0 or w_cat > 1:\n            continue\n        \n        ensemble_pred = w_xgb * xgb_val_preds + w_lgb * lgb_val_preds + w_cat * cat_val_preds\n        score = competition_metric(y_val, ensemble_pred, verbose=False)\n        \n        if score > best_score:\n            best_score = score\n            best_weights = (w_xgb, w_lgb, w_cat)\n            print(f\"  New best: XGB={w_xgb:.2f}, LGB={w_lgb:.2f}, CAT={w_cat:.2f}, score={score:.4f}\")\n\nprint(f\"\\n=== Best Ensemble ===\")\nprint(f\"XGBoost:  {best_weights[0]:.3f}\")\nprint(f\"LightGBM: {best_weights[1]:.3f}\")\nprint(f\"CatBoost: {best_weights[2]:.3f}\")\nprint(f\"Val Score: {best_score:.4f}\")\n\nfinal_val_pred = (\n    best_weights[0] * xgb_val_preds + \n    best_weights[1] * lgb_val_preds + \n    best_weights[2] * cat_val_preds\n)\n\nprint(\"\\n=== Validation Performance ===\")\ncompetition_metric(y_val, final_val_pred)\nprint(f\"MAE: {mean_absolute_error(y_val, final_val_pred):.2f}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing ensemble weights...\n",
      "  New best: XGB=0.00, LGB=0.00, CAT=1.00, score=0.4511\n",
      "  New best: XGB=0.00, LGB=0.30, CAT=0.70, score=0.4697\n",
      "  New best: XGB=0.00, LGB=0.40, CAT=0.60, score=0.4885\n",
      "  New best: XGB=0.00, LGB=0.50, CAT=0.50, score=0.5043\n",
      "  New best: XGB=0.00, LGB=0.60, CAT=0.40, score=0.5154\n",
      "  New best: XGB=0.00, LGB=0.70, CAT=0.30, score=0.5235\n",
      "\n",
      "=== Best Ensemble ===\n",
      "XGBoost:  0.000\n",
      "LightGBM: 0.700\n",
      "CatBoost: 0.300\n",
      "Val Score: 0.5235\n",
      "\n",
      "=== Validation Performance ===\n",
      "  APE > 100%: 23.2%\n",
      "  Valid samples: 76.8%\n",
      "  MAPE (valid): 0.3659\n",
      "  Scaled MAPE: 0.4765\n",
      "MAE: 10235.80\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Validation-Based Sector Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:22:21.757125Z",
     "start_time": "2025-10-05T17:22:21.747378Z"
    }
   },
   "source": [
    "# Analyze which sectors should be zeroed based on validation errors\n",
    "val_analysis = val_data[['sector', target]].copy()\n",
    "val_analysis['prediction'] = final_val_pred\n",
    "val_analysis['ape'] = np.abs(val_analysis[target] - val_analysis['prediction']) / np.maximum(val_analysis[target], 1e-10)\n",
    "\n",
    "# Identify sectors with consistently high errors OR consistently zero transactions\n",
    "sector_error_analysis = val_analysis.groupby('sector').agg({\n",
    "    target: ['mean', 'max', 'sum'],\n",
    "    'ape': 'mean',\n",
    "    'prediction': 'mean'\n",
    "}).reset_index()\n",
    "sector_error_analysis.columns = ['sector', 'target_mean', 'target_max', 'target_sum', 'ape_mean', 'pred_mean']\n",
    "\n",
    "# Sectors to zero:\n",
    "# 1. All transactions in val are zero\n",
    "# 2. OR mean APE > 2.0 (very bad predictions)\n",
    "zero_sectors = sector_error_analysis[\n",
    "    (sector_error_analysis['target_sum'] == 0) | \n",
    "    (sector_error_analysis['ape_mean'] > 2.0)\n",
    "]['sector'].tolist()\n",
    "\n",
    "print(f\"\\nSectors to zero (validation-based): {len(zero_sectors)}\")\n",
    "print(f\"Sectors: {sorted(zero_sectors)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sectors to zero (validation-based): 24\n",
      "Sectors: ['sector 12', 'sector 13', 'sector 17', 'sector 19', 'sector 26', 'sector 3', 'sector 33', 'sector 39', 'sector 40', 'sector 41', 'sector 42', 'sector 44', 'sector 49', 'sector 52', 'sector 53', 'sector 72', 'sector 73', 'sector 74', 'sector 75', 'sector 82', 'sector 87', 'sector 89', 'sector 95', 'sector 96']\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: Retrain on Full Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:23:08.420478Z",
     "start_time": "2025-10-05T17:22:21.805989Z"
    }
   },
   "source": "print(\"=== RETRAINING ON FULL DATA ===\")\n\n# Rebuild grid\nfull_grid = pd.DataFrame([(m, s) for m in months for s in sectors], columns=['month', 'sector'])\nfull_grid = full_grid.merge(new_house[['month', 'sector', target]], on=['month', 'sector'], how='left')\nfull_grid[target] = full_grid[target].fillna(0)\n\nfull_data = create_base_features(full_grid, target=target, sector_clusters=sector_clusters)\nfull_data = create_lag_features(full_data, target=target)\nfull_data = add_sector_stats(full_data, full_data, target=target)\n\nX_full = full_data[feature_cols].fillna(0)\ny_full = full_data[target]\n\nprint(f\"Full training set: {X_full.shape}\")\nprint(f\"Features: {len(feature_cols)}\")\n\n# Calculate caps\nhist_q99 = y_full.quantile(0.99)\nlast_train_month = full_data['month_date'].max()\nwindow_start = last_train_month - pd.DateOffset(months=6)\nrecent = full_data[full_data['month_date'] >= window_start]\nsector_recent_max = recent.groupby('sector')[target].max()\nsector_all_max = full_data.groupby('sector')[target].max()\nsector_cap_series = (1.2 * sector_recent_max).fillna(1.2 * sector_all_max)\nsector_caps = sector_cap_series.to_dict()\nglobal_cap = float(hist_q99 * 1.5)\n\nprint(f\"Global cap: {global_cap:.2f}\")\n\n# Create DMatrix for full data (for XGBoost)\ndfull = xgb.DMatrix(X_full, label=y_full)\n\n# Train final models\nprint(\"\\nTraining final XGBoost...\")\nfinal_xgb_models = []\nfor seed in seeds:\n    params = {\n        'max_depth': best_xgb_params['max_depth'],\n        'eta': best_xgb_params['learning_rate'],\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'gamma': 0.1,\n        'alpha': 0.1,\n        'lambda': 1.0,\n        'seed': seed\n    }\n    model = xgb.train(params, dfull, num_boost_round=best_xgb_params['n_estimators'],\n                     obj=xgb_adaptive_loss, verbose_eval=False)\n    final_xgb_models.append(model)\nprint(f\"  {len(final_xgb_models)} models ✓\")\n\nprint(\"\\nTraining final LightGBM...\")\nfinal_lgb_models = []\nfor seed in seeds:\n    model = lgb.LGBMRegressor(\n        n_estimators=best_lgb_params['n_estimators'],\n        max_depth=best_lgb_params['max_depth'],\n        learning_rate=best_lgb_params['learning_rate'],\n        num_leaves=2**best_lgb_params['max_depth'],\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        objective=lgb_adaptive_loss,\n        random_state=seed,\n        n_jobs=-1,\n        verbose=-1\n    )\n    model.fit(X_full, y_full)\n    final_lgb_models.append(model)\nprint(f\"  {len(final_lgb_models)} models ✓\")\n\nprint(\"\\nTraining final CatBoost...\")\nfinal_cat_models = []\nfor seed in seeds:\n    model = CatBoostRegressor(\n        iterations=best_cat_params['iterations'],\n        depth=best_cat_params['depth'],\n        learning_rate=best_cat_params['learning_rate'],\n        l2_leaf_reg=0.3,\n        random_seed=seed,\n        loss_function=CatBoostAdaptiveLoss(),\n        eval_metric='MAE',  # Use standard metric for final training\n        verbose=False\n    )\n    model.fit(X_full, y_full)\n    final_cat_models.append(model)\nprint(f\"  {len(final_cat_models)} models ✓\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RETRAINING ON FULL DATA ===\n",
      "Full training set: (7584, 289)\n",
      "Features: 289\n",
      "Global cap: 336131.37\n",
      "\n",
      "Training final XGBoost...\n",
      "  2 models ✓\n",
      "\n",
      "Training final LightGBM...\n",
      "  2 models ✓\n",
      "\n",
      "Training final CatBoost...\n",
      "  2 models ✓\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:23:12.887279Z",
     "start_time": "2025-10-05T17:23:08.629506Z"
    }
   },
   "source": "## ITERATIVE FORECASTING - Predict one month at a time, adding predictions to lag features\n\nprint(\"=\"*80)\nprint(\"ITERATIVE FORECASTING - Predicting 12 months sequentially\")\nprint(\"=\"*80)\n\n# Start with full training data up to Jul 2024\nforecast_data = full_data.copy()\n\n# Test months: Aug 2024 through Jul 2025\ntest_months_list = pd.date_range(start='2024-08-01', end='2025-07-01', freq='MS')\n\nall_predictions = []\n\nfor i, forecast_month in enumerate(test_months_list, 1):\n    forecast_month_str = forecast_month.strftime('%Y %b')\n    print(f\"\\n[{i}/12] Predicting {forecast_month_str}...\")\n    \n    # Get sectors to predict for this month\n    month_test = test[test['month'] == forecast_month_str].copy()\n    \n    # Create features for this month\n    month_test_data = create_base_features(month_test, target=target, sector_clusters=sector_clusters)\n    \n    # Build lag features using ACTUAL historical data + any previous predictions\n    combined_for_lags = pd.concat([\n        forecast_data[['month', 'sector', 'month_date', target]],\n        month_test_data[['month', 'sector', 'month_date']].assign(**{target: np.nan})\n    ]).sort_values(['sector', 'month_date']).reset_index(drop=True)\n    \n    combined_for_lags = create_lag_features(combined_for_lags, target=target)\n    month_features = combined_for_lags[combined_for_lags[target].isna()].reset_index(drop=True)\n    \n    # Transfer lag features\n    lag_cols = ['sector_target_encoded'] + \\\n               [f'amount_lag_{lag}' for lag in [1,2,3,4,5,6,7,9,12,18,24]] + \\\n               [f'amount_ewm_{span}' for span in [3,6,12]] + \\\n               [f'amount_rolling_mean_{w}' for w in [3,6,12]] + \\\n               [f'amount_rolling_std_{w}' for w in [3,6,12]] + \\\n               ['amount_lag_12_growth', 'trend_3m', 'trend_6m', 'volatility_6m',\n                'mom_change_1m', 'mom_change_3m', 'qoq_growth', 'acceleration_3m']\n    \n    for col in lag_cols:\n        month_test_data[col] = month_features[col].values\n    \n    # Add sector stats\n    month_test_data = add_sector_stats(month_test_data, forecast_data, target=target)\n    \n    # Prepare features\n    X_month = month_test_data[feature_cols].fillna(0)\n    \n    # Make predictions with ensemble\n    dmonth = xgb.DMatrix(X_month)\n    xgb_preds = np.maximum(np.mean([m.predict(dmonth) for m in final_xgb_models], axis=0), 0)\n    lgb_preds = np.maximum(np.mean([m.predict(X_month) for m in final_lgb_models], axis=0), 0)\n    cat_preds = np.maximum(np.mean([m.predict(X_month) for m in final_cat_models], axis=0), 0)\n    \n    month_pred = best_weights[0] * xgb_preds + best_weights[1] * lgb_preds + best_weights[2] * cat_preds\n    \n    # Post-processing for this month\n    # 1. Sector-wise caps\n    caps = np.array([sector_caps.get(s, global_cap) for s in month_test['sector']])\n    month_pred = np.minimum(month_pred, caps)\n    \n    # 2. Zero-history heuristic\n    lag_check_cols = [f'amount_lag_{k}' for k in [1,2,3,6]]\n    zero_hist_mask = month_test_data[lag_check_cols].fillna(0).sum(axis=1) == 0\n    month_pred[zero_hist_mask.values] = 0\n    \n    # 3. Manual sector zeroing (ONLY leader's list, NOT validation-based)\n    leader_zero_sectors = [11, 38, 40, 43, 48, 51, 52, 57, 71, 72, 73, 74, 81, 86, 88, 94, 95]\n    for sector_num in leader_zero_sectors:\n        sector_name = f'sector {sector_num}'\n        month_pred[month_test['sector'] == sector_name] = 0\n    \n    # REMOVED: Validation-based zeroing (was zeroing too many sectors)\n    # REMOVED: Uncertainty decay (was killing predictions for later months)\n    \n    print(f\"  Predictions: mean={month_pred.mean():.2f}, median={np.median(month_pred):.2f}, zeros={(month_pred==0).sum()}/{len(month_pred)}\")\n    \n    # Store predictions\n    month_results = month_test[['id', 'month', 'sector']].copy()\n    month_results['prediction'] = month_pred\n    all_predictions.append(month_results)\n    \n    # CRITICAL: Add predictions to forecast_data for next iteration\n    # This creates a feedback loop where future predictions use past predictions as features\n    new_rows = month_test_data[['month', 'sector', 'month_date']].copy()\n    new_rows[target] = month_pred\n    \n    # Add all feature columns needed\n    for col in forecast_data.columns:\n        if col not in new_rows.columns:\n            if col in month_test_data.columns:\n                new_rows[col] = month_test_data[col].values\n            else:\n                new_rows[col] = np.nan\n    \n    forecast_data = pd.concat([forecast_data, new_rows], ignore_index=True)\n    forecast_data = forecast_data.sort_values(['sector', 'month_date']).reset_index(drop=True)\n\n# Combine all monthly predictions\nfinal_predictions = pd.concat(all_predictions, ignore_index=True)\nfinal_predictions = final_predictions.sort_values('id').reset_index(drop=True)\n\nprint(f\"\\n{'='*80}\")\nprint(f\"ITERATIVE FORECASTING COMPLETE\")\nprint(f\"{'='*80}\")\nprint(f\"Total predictions: {len(final_predictions)}\")\nprint(f\"Predicted range: {final_predictions['prediction'].min():.2f} to {final_predictions['prediction'].max():.2f}\")\nprint(f\"Predicted mean: {final_predictions['prediction'].mean():.2f}\")\nprint(f\"Zeros: {(final_predictions['prediction'] == 0).sum()} / {len(final_predictions)} ({(final_predictions['prediction'] == 0).sum()/len(final_predictions)*100:.1f}%)\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ITERATIVE FORECASTING - Predicting 12 months sequentially\n",
      "================================================================================\n",
      "\n",
      "[1/12] Predicting 2024 Aug...\n",
      "  Predictions: mean=8969.59, median=4176.58, zeros=22/96\n",
      "\n",
      "[2/12] Predicting 2024 Sep...\n",
      "  Predictions: mean=6990.21, median=3905.32, zeros=23/96\n",
      "\n",
      "[3/12] Predicting 2024 Oct...\n",
      "  Predictions: mean=5576.52, median=3331.70, zeros=22/96\n",
      "\n",
      "[4/12] Predicting 2024 Nov...\n",
      "  Predictions: mean=5012.90, median=2674.55, zeros=24/96\n",
      "\n",
      "[5/12] Predicting 2024 Dec...\n",
      "  Predictions: mean=6140.97, median=3666.70, zeros=23/96\n",
      "\n",
      "[6/12] Predicting 2025 Jan...\n",
      "  Predictions: mean=4628.27, median=2506.69, zeros=24/96\n",
      "\n",
      "[7/12] Predicting 2025 Feb...\n",
      "  Predictions: mean=3454.06, median=1610.67, zeros=35/96\n",
      "\n",
      "[8/12] Predicting 2025 Mar...\n",
      "  Predictions: mean=3362.90, median=858.80, zeros=34/96\n",
      "\n",
      "[9/12] Predicting 2025 Apr...\n",
      "  Predictions: mean=3037.49, median=873.54, zeros=33/96\n",
      "\n",
      "[10/12] Predicting 2025 May...\n",
      "  Predictions: mean=2781.83, median=664.44, zeros=36/96\n",
      "\n",
      "[11/12] Predicting 2025 Jun...\n",
      "  Predictions: mean=2651.05, median=607.71, zeros=35/96\n",
      "\n",
      "[12/12] Predicting 2025 Jul...\n",
      "  Predictions: mean=2572.53, median=518.53, zeros=35/96\n",
      "\n",
      "================================================================================\n",
      "ITERATIVE FORECASTING COMPLETE\n",
      "================================================================================\n",
      "Total predictions: 1152\n",
      "Predicted range: 0.00 to 55662.66\n",
      "Predicted mean: 4598.19\n",
      "Zeros: 346 / 1152 (30.0%)\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "source": "## STEP 8: Iterative Forecasting (1-month ahead, 12 iterations)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## STEP 9: Prepare Final Submission",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:23:13.049322Z",
     "start_time": "2025-10-05T17:23:13.045192Z"
    }
   },
   "source": "# No temporal smoothing needed - iterative forecasting already accounts for temporal dependencies\n\ntest_pred = final_predictions['prediction'].values\n\nprint(f\"\\nFinal Predictions (from iterative forecasting):\")\nprint(f\"  Range: {test_pred.min():.2f} to {test_pred.max():.2f}\")\nprint(f\"  Mean: {test_pred.mean():.2f}\")\nprint(f\"  Median: {np.median(test_pred):.2f}\")\nprint(f\"  Zeros: {(test_pred == 0).sum()} / {len(test_pred)} ({(test_pred == 0).sum()/len(test_pred)*100:.1f}%)\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Predictions (from iterative forecasting):\n",
      "  Range: 0.00 to 55662.66\n",
      "  Mean: 4598.19\n",
      "  Median: 1989.85\n",
      "  Zeros: 346 / 1152 (30.0%)\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-05T17:23:13.109754Z",
     "start_time": "2025-10-05T17:23:13.097621Z"
    }
   },
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'new_house_transaction_amount': test_pred\n",
    "})\n",
    "\n",
    "orig_test = pd.read_csv(file_path + 'test.csv')\n",
    "assert submission['id'].tolist() == orig_test['id'].tolist(), \"ID order mismatch!\"\n",
    "assert np.isfinite(submission['new_house_transaction_amount']).all(), \"Non-finite predictions!\"\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"✅ Submission saved to submission.csv\")\n",
    "\n",
    "print(\"\\nFirst 10:\")\n",
    "print(submission.head(10))\n",
    "print(\"\\nLast 10:\")\n",
    "print(submission.tail(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mode: {'TEST' if TEST_MODE else 'PRODUCTION'}\")\n",
    "print(f\"Best XGBoost: {best_xgb_params}, score: {best_xgb_score:.4f}\")\n",
    "print(f\"Best LightGBM: {best_lgb_params}, score: {best_lgb_score:.4f}\")\n",
    "print(f\"Best CatBoost: {best_cat_params}, score: {best_cat_score:.4f}\")\n",
    "print(f\"Ensemble weights: XGB={best_weights[0]:.3f}, LGB={best_weights[1]:.3f}, CAT={best_weights[2]:.3f}\")\n",
    "print(f\"Validation score: {best_score:.4f}\")\n",
    "print(f\"Sectors auto-zeroed: {len(zero_sectors)}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission saved to submission.csv\n",
      "\n",
      "First 10:\n",
      "                   id  new_house_transaction_amount\n",
      "0   2024 Aug_sector 1                   6063.068469\n",
      "1   2024 Aug_sector 2                  17055.620439\n",
      "2   2024 Aug_sector 3                      0.000000\n",
      "3   2024 Aug_sector 4                    565.776000\n",
      "4   2024 Aug_sector 5                   3352.693035\n",
      "5   2024 Aug_sector 6                  16464.841388\n",
      "6   2024 Aug_sector 7                  15837.944512\n",
      "7   2024 Aug_sector 8                  19115.401146\n",
      "8   2024 Aug_sector 9                   3837.228000\n",
      "9  2024 Aug_sector 10                  16447.806999\n",
      "\n",
      "Last 10:\n",
      "                      id  new_house_transaction_amount\n",
      "1142  2025 Jul_sector 87                      0.000000\n",
      "1143  2025 Jul_sector 88                    655.793147\n",
      "1144  2025 Jul_sector 89                   2577.240913\n",
      "1145  2025 Jul_sector 90                     41.020657\n",
      "1146  2025 Jul_sector 91                   3523.717828\n",
      "1147  2025 Jul_sector 92                  11551.756311\n",
      "1148  2025 Jul_sector 93                  15259.485844\n",
      "1149  2025 Jul_sector 94                      0.000000\n",
      "1150  2025 Jul_sector 95                      0.000000\n",
      "1151  2025 Jul_sector 96                      0.000000\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Mode: TEST\n",
      "Best XGBoost: {'max_depth': 5, 'subsample': 0.7075688195363431, 'colsample_bytree': 0.8595248529280993, 'gamma': 0.1670166652998772, 'alpha': 0.1860811063711681, 'lambda': 1.8621618644332167, 'n_estimators': 665, 'learning_rate': 0.00597986549809105}, score: 0.5053\n",
      "Best LightGBM: {'max_depth': 10, 'learning_rate': 0.013790438408195164, 'num_leaves': 118, 'subsample': 0.8487802322757152, 'colsample_bytree': 0.8384259258595648, 'reg_alpha': 0.1538328052586716, 'reg_lambda': 0.5506559891687581, 'n_estimators': 147}, score: 0.5176\n",
      "Best CatBoost: {'depth': 5, 'learning_rate': 0.022658164793916934, 'l2_leaf_reg': 0.23438768621794598, 'iterations': 499}, score: 0.4562\n",
      "Ensemble weights: XGB=0.000, LGB=0.700, CAT=0.300\n",
      "Validation score: 0.5235\n",
      "Sectors auto-zeroed: 24\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 78
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
