{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Model Ensemble Approach\n\nCleaner workflow:\n1. Split data into train/validation\n2. Tune hyperparameters for each model type (XGBoost, LightGBM, CatBoost)\n3. Generate EWGM (Exponentially Weighted Geometric Mean) predictions\n4. Find optimal ensemble weights across all 4 models\n5. Retrain everything on full data\n6. Predict on test set with weighted ensemble"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:08:27.767157Z",
     "start_time": "2025-10-07T09:08:27.765334Z"
    }
   },
   "source": [
    "# Quick test mode - reduces iterations/models for faster iteration\n",
    "# Set to False for final submission run\n",
    "TEST_MODE = False"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:08:27.826336Z",
     "start_time": "2025-10-07T09:08:27.820791Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Running in {'TEST' if TEST_MODE else 'PRODUCTION'} mode\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in PRODUCTION mode\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": "## EWGM (Exponentially Weighted Geometric Mean) Implementation\n\nA statistical model that uses exponentially weighted geometric means to predict future values based on historical patterns. Works particularly well for capturing seasonality effects like December spikes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_month_codes():\n    \"\"\"Map month abbreviations to numbers\"\"\"\n    return {\n        'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4,\n        'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8,\n        'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n    }\n\ndef split_test_id_column(df):\n    \"\"\"Split test ID into month_text and sector\"\"\"\n    parts = df.id.str.split('_', expand=True)\n    df['month_text'] = parts[0]\n    df['sector'] = parts[1]\n    return df\n\ndef add_time_and_sector_fields(df, month_codes):\n    \"\"\"Add time and sector_id fields\"\"\"\n    # Extract sector_id from sector column\n    if 'sector' in df.columns and 'sector_id' not in df.columns:\n        if df['sector'].dtype == 'object' and 'sector' in df['sector'].iloc[0]:\n            df['sector_id'] = df.sector.str.slice(7, None).astype(int)\n        else:\n            # If sector is just the ID, use it directly\n            df['sector_id'] = df['sector'].astype(int) if df['sector'].dtype != 'int64' else df['sector']\n    \n    # Handle month_text column - create it if it doesn't exist\n    if 'month_text' not in df.columns:\n        # If we have 'month' column in format '2019-Jan', use it\n        if 'month' in df.columns:\n            df['month_text'] = df['month']\n    \n    # Extract month number and year\n    if 'month' not in df.columns or df['month'].dtype == 'object':\n        # Parse from month_text\n        df['month_num'] = df['month_text'].str.slice(5, None).map(month_codes)\n        df['year'] = df['month_text'].str.slice(0, 4).astype(int)\n    else:\n        # Already have numeric month\n        df['month_num'] = df['month']\n        if 'year' not in df.columns:\n            df['year'] = df['month_text'].str.slice(0, 4).astype(int)\n    \n    df['time'] = (df['year'] - 2019) * 12 + df['month_num'] - 1\n    \n    return df\n\ndef build_amount_matrix(train_nht, month_codes):\n    \"\"\"\n    Build a matrix of transaction amounts: rows=time, columns=sector_id\n    Missing sector-month combinations are filled with 0\n    \"\"\"\n    train_nht = train_nht.copy()\n    \n    # Ensure we have month_text column\n    if 'month_text' not in train_nht.columns and 'month' in train_nht.columns:\n        train_nht['month_text'] = train_nht['month']\n    \n    train_nht = add_time_and_sector_fields(train_nht, month_codes)\n    \n    # Pivot: time as index, sector_id as columns\n    pivot = train_nht.set_index(['time', 'sector_id']).amount_new_house_transactions.unstack()\n    pivot = pivot.fillna(0)\n    \n    # Ensure all 96 sectors exist\n    all_sectors = np.arange(1, 97)\n    for s in all_sectors:\n        if s not in pivot.columns:\n            pivot[s] = 0\n    \n    pivot = pivot[all_sectors]\n    return pivot\n\ndef compute_december_multipliers(a_tr, eps=1e-9, min_dec_obs=1, clip_low=0.8, clip_high=1.5):\n    \"\"\"\n    Compute sector-specific December multipliers\n    December months have time % 12 == 11\n    \"\"\"\n    is_december = (a_tr.index.values % 12) == 11\n    dec_counts = a_tr[is_december].astype(bool).sum(axis=0)\n    \n    dec_means = a_tr[is_december].mean(axis=0)\n    nondec_means = a_tr[~is_december].mean(axis=0)\n    \n    raw_mult = dec_means / (nondec_means + eps)\n    overall_mult = float(dec_means.mean() / (nondec_means.mean() + eps))\n    \n    # Use overall multiplier where insufficient December observations\n    raw_mult = raw_mult.where(dec_counts >= min_dec_obs, overall_mult)\n    \n    # Replace inf/-inf with 1.0\n    raw_mult = raw_mult.replace([np.inf, -np.inf], 1.0).fillna(1.0)\n    \n    # Clip to reasonable range\n    clipped_mult = raw_mult.clip(lower=clip_low, upper=clip_high)\n    \n    return clipped_mult.to_dict()\n\ndef apply_december_bump(a_pred, sector_to_mult):\n    \"\"\"Apply December multipliers to predictions\"\"\"\n    dec_rows = [t for t in a_pred.index.values if (t % 12) == 11]\n    \n    if len(dec_rows) == 0:\n        return a_pred\n    \n    for sector in a_pred.columns:\n        m = sector_to_mult.get(sector, 1.0)\n        a_pred.loc[dec_rows, sector] = a_pred.loc[dec_rows, sector] * m\n    \n    return a_pred\n\ndef ewgm_per_sector(a_tr, sector, n_lags, alpha):\n    \"\"\"\n    Calculate Exponentially Weighted Geometric Mean for one sector\n    \n    EWGM = exp(sum(weights * log(values)))\n    \n    Parameters:\n    - a_tr: training amount matrix\n    - sector: sector ID\n    - n_lags: number of historical values to use\n    - alpha: decay parameter (0-1), higher = more weight on recent\n    \"\"\"\n    # Calculate exponential weights\n    weights = np.array([alpha**(n_lags - 1 - i) for i in range(n_lags)], dtype=float)\n    weights = weights / weights.sum()  # Normalize\n    \n    # Get historical values\n    values = a_tr[sector].values\n    \n    # Take last n_lags observations\n    if len(values) < n_lags:\n        values = np.pad(values, (n_lags - len(values), 0), constant_values=0)\n    else:\n        values = values[-n_lags:]\n    \n    # Add epsilon to avoid log(0)\n    values = values + 1e-10\n    \n    # Compute EWGM\n    log_values = np.log(values)\n    ewgm = np.exp(np.dot(weights, log_values))\n    \n    return ewgm\n\ndef generate_ewgm_predictions(train_data, test_data, n_lags=6, alpha=0.9):\n    \"\"\"\n    Generate EWGM predictions for test set\n    \n    Parameters:\n    - train_data: training DataFrame with amount_new_house_transactions\n    - test_data: test DataFrame with id column\n    - n_lags: number of lags to use (default: 6)\n    - alpha: exponential decay parameter (default: 0.9)\n    \n    Returns:\n    - predictions: array of predictions matching test_data order\n    \"\"\"\n    month_codes = build_month_codes()\n    \n    # Build amount matrix from training data\n    a_tr = build_amount_matrix(train_data, month_codes)\n    \n    # Compute December multipliers\n    sector_to_mult = compute_december_multipliers(a_tr)\n    \n    # Prepare test data\n    test_df = test_data.copy()\n    test_df = split_test_id_column(test_df)\n    test_df = add_time_and_sector_fields(test_df, month_codes)\n    \n    # Generate predictions\n    predictions = []\n    \n    for idx, row in test_df.iterrows():\n        sector_id = row['sector_id']\n        time = row['time']\n        \n        # Calculate EWGM for this sector\n        ewgm_pred = ewgm_per_sector(a_tr, sector_id, n_lags, alpha)\n        \n        # Apply December bump if applicable\n        if (time % 12) == 11:  # December\n            mult = sector_to_mult.get(sector_id, 1.0)\n            ewgm_pred = ewgm_pred * mult\n        \n        predictions.append(ewgm_pred)\n    \n    return np.array(predictions)\n\nprint(\"EWGM functions defined\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:08:27.890756Z",
     "start_time": "2025-10-07T09:08:27.877151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EWGM functions defined\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition Metric Implementation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:08:27.951061Z",
     "start_time": "2025-10-07T09:08:27.946536Z"
    }
   },
   "source": [
    "def competition_metric(y_true, y_pred, eps=1e-10, verbose=True):\n",
    "    \"\"\"\n",
    "    Two-stage metric from competition rules:\n",
    "    - Stage 1: If >30% samples have APE>100%, score=0\n",
    "    - Stage 2: Otherwise, calculate scaled MAPE on samples with APE<=100%\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    \n",
    "    # Absolute percentage error for each sample\n",
    "    ape = np.abs(y_true - y_pred) / np.maximum(y_true, eps)\n",
    "    \n",
    "    # Stage 1 check\n",
    "    extreme_errors = (ape > 1.0).sum() / len(ape)\n",
    "    if verbose:\n",
    "        print(f\"  APE > 100%: {extreme_errors*100:.1f}%\")\n",
    "    \n",
    "    if extreme_errors > 0.3:\n",
    "        if verbose:\n",
    "            print(\"  FAILED Stage 1\")\n",
    "        return 0.0\n",
    "    \n",
    "    # Stage 2: scaled MAPE on good predictions only\n",
    "    valid_mask = ape <= 1.0\n",
    "    if valid_mask.sum() == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    mape = ape[valid_mask].mean()\n",
    "    fraction_valid = valid_mask.sum() / len(ape)\n",
    "    scaled_mape = mape / fraction_valid\n",
    "    score = max(0, 1 - scaled_mape)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Valid samples: {fraction_valid*100:.1f}%\")\n",
    "        print(f\"  MAPE (valid): {mape:.4f}\")\n",
    "        print(f\"  Scaled MAPE: {scaled_mape:.4f}\")\n",
    "    \n",
    "    return score"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:08:28.035509Z",
     "start_time": "2025-10-07T09:08:27.999332Z"
    }
   },
   "source": [
    "file_path = \"./data/\"\n",
    "\n",
    "# All the training data files\n",
    "new_house = pd.read_csv(file_path + 'train/new_house_transactions.csv')\n",
    "new_house_nearby = pd.read_csv(file_path + 'train/new_house_transactions_nearby_sectors.csv')\n",
    "pre_owned = pd.read_csv(file_path + 'train/pre_owned_house_transactions.csv')\n",
    "pre_owned_nearby = pd.read_csv(file_path + 'train/pre_owned_house_transactions_nearby_sectors.csv')\n",
    "land_trans = pd.read_csv(file_path + 'train/land_transactions.csv')\n",
    "land_trans_nearby = pd.read_csv(file_path + 'train/land_transactions_nearby_sectors.csv')\n",
    "sector_poi = pd.read_csv(file_path + 'train/sector_POI.csv')\n",
    "city_indexes = pd.read_csv(file_path + 'train/city_indexes.csv')\n",
    "search_index = pd.read_csv(file_path + 'train/city_search_index.csv')\n",
    "test = pd.read_csv(file_path + 'test.csv')\n",
    "test[['month', 'sector']] = test['id'].str.split('_', n=1, expand=True)\n",
    "\n",
    "print(f\"Training samples: {len(new_house)}\")\n",
    "print(f\"Test samples: {len(test)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5433\n",
      "Test samples: 1152\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Function\n",
    "\n",
    "Putting this in a function so I can reuse it for train/val/test without code duplication"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:08:28.060488Z",
     "start_time": "2025-10-07T09:08:28.051271Z"
    }
   },
   "source": [
    "def create_base_features(df, target='amount_new_house_transactions'):\n",
    "    \"\"\"\n",
    "    Merge all auxiliary datasets and create basic features\n",
    "    (everything except lag features which need special handling)\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Merge all the datasets\n",
    "    data = data.merge(new_house_nearby, on=['month', 'sector'], how='left')\n",
    "    data = data.merge(pre_owned, on=['month', 'sector'], how='left')\n",
    "    data = data.merge(pre_owned_nearby, on=['month', 'sector'], how='left')\n",
    "    data = data.merge(land_trans, on=['month', 'sector'], how='left')\n",
    "    data = data.merge(land_trans_nearby, on=['month', 'sector'], how='left')\n",
    "    data = data.merge(sector_poi, on='sector', how='left')\n",
    "    \n",
    "    # DateTime features\n",
    "    data['month_date'] = pd.to_datetime(data['month'], format='%Y-%b' if 'id' not in data.columns else '%Y %b')\n",
    "    data['year'] = data['month_date'].dt.year\n",
    "    data['month_num'] = data['month_date'].dt.month\n",
    "    data['quarter'] = data['month_date'].dt.quarter\n",
    "    \n",
    "    # City-level economic indicators\n",
    "    data = data.merge(city_indexes, left_on='year', right_on='city_indicator_data_year', how='left')\n",
    "    \n",
    "    # Search volume aggregates by month\n",
    "    search_agg = search_index.groupby('month').agg({\n",
    "        'search_volume': ['sum', 'mean', 'max', 'std']\n",
    "    }).reset_index()\n",
    "    search_agg.columns = ['month', 'search_volume_sum', 'search_volume_mean', \n",
    "                          'search_volume_max', 'search_volume_std']\n",
    "    data = data.merge(search_agg, on='month', how='left')\n",
    "    \n",
    "    # Cyclic encoding for seasonality - helps model understand Dec is close to Jan\n",
    "    data['month_sin'] = np.sin(2 * np.pi * data['month_num'] / 12)\n",
    "    data['month_cos'] = np.cos(2 * np.pi * data['month_num'] / 12)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_lag_features(data, target='amount_new_house_transactions'):\n",
    "    \"\"\"\n",
    "    Create time-based lag features\n",
    "    Must be called AFTER sorting by sector and date\n",
    "    \"\"\"\n",
    "    data = data.sort_values(['sector', 'month_date']).copy()\n",
    "    \n",
    "    # Target encoding - expanding mean (prevents leakage with shift)\n",
    "    data['sector_target_encoded'] = data.groupby('sector')[target].transform(\n",
    "        lambda x: x.shift(1).expanding().mean()\n",
    "    )\n",
    "    \n",
    "    # Simple lags - what happened N months ago?\n",
    "    for lag in [1, 2, 3, 4, 6, 12]:\n",
    "        data[f'amount_lag_{lag}'] = data.groupby('sector')[target].shift(lag)\n",
    "    \n",
    "    # Exponential moving averages - recent values weighted more\n",
    "    for span in [3, 6, 12]:\n",
    "        data[f'amount_ewm_{span}'] = data.groupby('sector')[target].transform(\n",
    "            lambda x: x.shift(1).ewm(span=span, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    # Rolling statistics - trends and volatility\n",
    "    for window in [3, 6, 12]:\n",
    "        data[f'amount_rolling_mean_{window}'] = data.groupby('sector')[target].transform(\n",
    "            lambda x: x.shift(1).rolling(window, min_periods=1).mean()\n",
    "        )\n",
    "        data[f'amount_rolling_std_{window}'] = data.groupby('sector')[target].transform(\n",
    "            lambda x: x.shift(1).rolling(window, min_periods=1).std()\n",
    "        )\n",
    "    \n",
    "    # Year-over-year growth rate\n",
    "    data['amount_lag_12_growth'] = (\n",
    "        data.groupby('sector')[target].shift(1) - data.groupby('sector')[target].shift(13)\n",
    "    ) / (data.groupby('sector')[target].shift(13) + 1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def add_sector_stats(data, train_data, target='amount_new_house_transactions'):\n",
    "    \"\"\"\n",
    "    Add sector-level statistics calculated from training data only\n",
    "    \"\"\"\n",
    "    sector_stats = train_data.groupby('sector')[target].agg(['mean', 'std']).reset_index()\n",
    "    sector_stats.columns = ['sector', 'sector_mean', 'sector_std']\n",
    "    data = data.merge(sector_stats, on='sector', how='left')\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"Feature engineering functions defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering functions defined\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Create Train/Validation Split\n",
    "\n",
    "Using last 6 months as validation to simulate the actual test scenario"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:08:28.489366Z",
     "start_time": "2025-10-07T09:08:28.111859Z"
    }
   },
   "source": [
    "target = 'amount_new_house_transactions'\n",
    "\n",
    "# Changed: build dense sector×month grid including zero targets\n",
    "months = sorted(set(new_house['month']) | set(pre_owned['month']) | set(land_trans['month']))\n",
    "sectors = sorted(set(new_house['sector']) | set(pre_owned['sector']) | set(land_trans['sector']))\n",
    "train_grid = pd.DataFrame([(m, s) for m in months for s in sectors], columns=['month', 'sector'])\n",
    "\n",
    "# Attach target (zeros when missing)\n",
    "train_grid = train_grid.merge(new_house[['month', 'sector', target]], on=['month', 'sector'], how='left')\n",
    "train_grid[target] = train_grid[target].fillna(0)\n",
    "\n",
    "# Start with base features on the grid (not just observed rows)\n",
    "all_data = create_base_features(train_grid, target=target)\n",
    "\n",
    "# Time-based split - last 6 months for validation\n",
    "val_months = 6\n",
    "val_cutoff = all_data['month_date'].max() - pd.DateOffset(months=val_months)\n",
    "\n",
    "train_data = all_data[all_data['month_date'] < val_cutoff].copy()\n",
    "val_data = all_data[all_data['month_date'] >= val_cutoff].copy()\n",
    "\n",
    "print(f\"Train: {len(train_data)} samples, {train_data['month_date'].min()} to {train_data['month_date'].max()}\")\n",
    "print(f\"Val:   {len(val_data)} samples, {val_data['month_date'].min()} to {val_data['month_date'].max()}\")\n",
    "\n",
    "# Create lag features separately for train and val\n",
    "# For train: only use train data\n",
    "train_data = create_lag_features(train_data, target=target)\n",
    "\n",
    "# For val: concatenate train+val, create lags, then extract val portion\n",
    "# This way validation lags can use recent training data\n",
    "combined = pd.concat([train_data, val_data]).sort_values(['sector', 'month_date'])\n",
    "combined = create_lag_features(combined, target=target)\n",
    "val_data = combined[combined['month_date'] >= val_cutoff].copy()\n",
    "\n",
    "# Add sector statistics (calculated only from training data)\n",
    "train_data = add_sector_stats(train_data, train_data, target=target)\n",
    "val_data = add_sector_stats(val_data, train_data, target=target)\n",
    "\n",
    "# Define features to use (exclude target and metadata)\n",
    "exclude_cols = [\n",
    "    'month', 'sector', 'month_date', target, 'city_indicator_data_year',\n",
    "    'num_new_house_transactions', 'area_new_house_transactions', 'price_new_house_transactions',\n",
    "    'area_per_unit_new_house_transactions', 'total_price_per_unit_new_house_transactions',\n",
    "    'num_new_house_available_for_sale', 'area_new_house_available_for_sale',\n",
    "    'period_new_house_sell_through'\n",
    "]\n",
    "feature_cols = [col for col in train_data.columns\n",
    "                if col not in exclude_cols and train_data[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "X_train = train_data[feature_cols].fillna(0)\n",
    "y_train = train_data[target]\n",
    "X_val = val_data[feature_cols].fillna(0)\n",
    "y_val = val_data[target]\n",
    "\n",
    "print(f\"\\nFeatures: {len(feature_cols)}\")\n",
    "print(f\"X_train: {X_train.shape}, X_val: {X_val.shape}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6912 samples, 2019-01-01 00:00:00 to 2023-12-01 00:00:00\n",
      "Val:   672 samples, 2024-01-01 00:00:00 to 2024-07-01 00:00:00\n",
      "\n",
      "Features: 264\n",
      "X_train: (6912, 264), X_val: (672, 264)\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Hyperparameter Tuning\n",
    "\n",
    "Grid search for each model type using validation set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:38:24.994882Z",
     "start_time": "2025-10-07T09:08:28.494782Z"
    }
   },
   "source": [
    "# Reduce search space in test mode\n",
    "if TEST_MODE:\n",
    "    print(\"[TEST MODE] Using reduced hyperparameter search\")\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [500],\n",
    "        'max_depth': [7],\n",
    "        'learning_rate': [0.02]\n",
    "    }\n",
    "    lgb_param_grid = {\n",
    "        'n_estimators': [500],\n",
    "        'max_depth': [7],\n",
    "        'learning_rate': [0.02]\n",
    "    }\n",
    "    cat_param_grid = {\n",
    "        'iterations': [500],\n",
    "        'depth': [7],\n",
    "        'learning_rate': [0.03]\n",
    "    }\n",
    "else:\n",
    "    print(\"[PRODUCTION MODE] Using full hyperparameter search\")\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [1500, 2000, 2500],\n",
    "        'max_depth': [7, 8, 9],\n",
    "        'learning_rate': [0.01, 0.015, 0.02]\n",
    "    }\n",
    "    lgb_param_grid = {\n",
    "        'n_estimators': [1500, 2000, 2500],\n",
    "        'max_depth': [7, 8, 9],\n",
    "        'learning_rate': [0.01, 0.015, 0.02]\n",
    "    }\n",
    "    cat_param_grid = {\n",
    "        'iterations': [1000, 1500, 2000],\n",
    "        'depth': [7, 8, 9],\n",
    "        'learning_rate': [0.02, 0.03, 0.04]\n",
    "    }\n",
    "\n",
    "# Tune XGBoost\n",
    "print(\"\\nTuning XGBoost...\")\n",
    "best_xgb_score = -1\n",
    "best_xgb_params = None\n",
    "\n",
    "for n_est in xgb_param_grid['n_estimators']:\n",
    "    for depth in xgb_param_grid['max_depth']:\n",
    "        for lr in xgb_param_grid['learning_rate']:\n",
    "            model = xgb.XGBRegressor(\n",
    "                n_estimators=n_est,\n",
    "                max_depth=depth,\n",
    "                learning_rate=lr,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            model.fit(X_train, y_train, verbose=False)\n",
    "            pred = model.predict(X_val)\n",
    "            score = competition_metric(y_val, pred, verbose=False)\n",
    "            \n",
    "            if score > best_xgb_score:\n",
    "                best_xgb_score = score\n",
    "                best_xgb_params = {'n_estimators': n_est, 'max_depth': depth, 'learning_rate': lr}\n",
    "                print(f\"  New best: {best_xgb_params}, score: {best_xgb_score:.4f}\")\n",
    "\n",
    "print(f\"\\nBest XGBoost: {best_xgb_params}, score: {best_xgb_score:.4f}\")\n",
    "\n",
    "# Tune LightGBM\n",
    "print(\"\\nTuning LightGBM...\")\n",
    "best_lgb_score = -1\n",
    "best_lgb_params = None\n",
    "\n",
    "for n_est in lgb_param_grid['n_estimators']:\n",
    "    for depth in lgb_param_grid['max_depth']:\n",
    "        for lr in lgb_param_grid['learning_rate']:\n",
    "            model = lgb.LGBMRegressor(\n",
    "                n_estimators=n_est,\n",
    "                max_depth=depth,\n",
    "                learning_rate=lr,\n",
    "                num_leaves=2**depth,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbose=-1\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_val)\n",
    "            score = competition_metric(y_val, pred, verbose=False)\n",
    "            \n",
    "            if score > best_lgb_score:\n",
    "                best_lgb_score = score\n",
    "                best_lgb_params = {'n_estimators': n_est, 'max_depth': depth, 'learning_rate': lr}\n",
    "                print(f\"  New best: {best_lgb_params}, score: {best_lgb_score:.4f}\")\n",
    "\n",
    "print(f\"\\nBest LightGBM: {best_lgb_params}, score: {best_lgb_score:.4f}\")\n",
    "\n",
    "# Tune CatBoost\n",
    "print(\"\\nTuning CatBoost...\")\n",
    "best_cat_score = -1\n",
    "best_cat_params = None\n",
    "\n",
    "for n_est in cat_param_grid['iterations']:\n",
    "    for depth in cat_param_grid['depth']:\n",
    "        for lr in cat_param_grid['learning_rate']:\n",
    "            model = CatBoostRegressor(\n",
    "                iterations=n_est,\n",
    "                depth=depth,\n",
    "                learning_rate=lr,\n",
    "                l2_leaf_reg=3.0,\n",
    "                random_seed=42,\n",
    "                loss_function='MAE',\n",
    "                verbose=False\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_val)\n",
    "            score = competition_metric(y_val, pred, verbose=False)\n",
    "            \n",
    "            if score > best_cat_score:\n",
    "                best_cat_score = score\n",
    "                best_cat_params = {'iterations': n_est, 'depth': depth, 'learning_rate': lr}\n",
    "                print(f\"  New best: {best_cat_params}, score: {best_cat_score:.4f}\")\n",
    "\n",
    "print(f\"\\nBest CatBoost: {best_cat_params}, score: {best_cat_score:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRODUCTION MODE] Using full hyperparameter search\n",
      "\n",
      "Tuning XGBoost...\n",
      "  New best: {'n_estimators': 1500, 'max_depth': 7, 'learning_rate': 0.01}, score: 0.0000\n",
      "\n",
      "Best XGBoost: {'n_estimators': 1500, 'max_depth': 7, 'learning_rate': 0.01}, score: 0.0000\n",
      "\n",
      "Tuning LightGBM...\n",
      "  New best: {'n_estimators': 1500, 'max_depth': 7, 'learning_rate': 0.01}, score: 0.0000\n",
      "\n",
      "Best LightGBM: {'n_estimators': 1500, 'max_depth': 7, 'learning_rate': 0.01}, score: 0.0000\n",
      "\n",
      "Tuning CatBoost...\n",
      "  New best: {'iterations': 1000, 'depth': 7, 'learning_rate': 0.02}, score: 0.5070\n",
      "  New best: {'iterations': 1000, 'depth': 8, 'learning_rate': 0.02}, score: 0.5088\n",
      "  New best: {'iterations': 1000, 'depth': 9, 'learning_rate': 0.04}, score: 0.5117\n",
      "  New best: {'iterations': 2000, 'depth': 7, 'learning_rate': 0.02}, score: 0.5122\n",
      "\n",
      "Best CatBoost: {'iterations': 2000, 'depth': 7, 'learning_rate': 0.02}, score: 0.5122\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Train Models with Best Params\n",
    "\n",
    "Create ensemble of models using best hyperparameters found above"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:41:30.860449Z",
     "start_time": "2025-10-07T09:38:25.216225Z"
    }
   },
   "source": "# Reduce ensemble size in test mode\nif TEST_MODE:\n    print(\"[TEST MODE] Training 2 models per algorithm\")\n    seeds = [42, 123]\nelse:\n    print(\"[PRODUCTION MODE] Training 5 models per algorithm\")\n    seeds = [42, 123, 456, 789, 2024]\n\n# Train XGBoost ensemble\nprint(\"\\nTraining XGBoost models...\")\nxgb_models = []\nfor seed in seeds:\n    model = xgb.XGBRegressor(\n        n_estimators=best_xgb_params['n_estimators'],\n        max_depth=best_xgb_params['max_depth'],\n        learning_rate=best_xgb_params['learning_rate'],\n        subsample=0.8,\n        colsample_bytree=0.8,\n        gamma=0.1,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=seed,\n        n_jobs=-1\n    )\n    model.fit(X_train, y_train, verbose=False)\n    xgb_models.append(model)\n    print(f\"  XGB seed {seed} trained\")\n\n# Train LightGBM ensemble\nprint(\"\\nTraining LightGBM models...\")\nlgb_models = []\nfor seed in seeds:\n    model = lgb.LGBMRegressor(\n        n_estimators=best_lgb_params['n_estimators'],\n        max_depth=best_lgb_params['max_depth'],\n        learning_rate=best_lgb_params['learning_rate'],\n        num_leaves=2**best_lgb_params['max_depth'],\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_alpha=0.1,\n        reg_lambda=1.0,\n        random_state=seed,\n        n_jobs=-1,\n        verbose=-1\n    )\n    model.fit(X_train, y_train)\n    lgb_models.append(model)\n    print(f\"  LGB seed {seed} trained\")\n\n# Train CatBoost ensemble\nprint(\"\\nTraining CatBoost models...\")\ncat_models = []\nfor seed in seeds:\n    model = CatBoostRegressor(\n        iterations=best_cat_params['iterations'],\n        depth=best_cat_params['depth'],\n        learning_rate=best_cat_params['learning_rate'],\n        l2_leaf_reg=3.0,\n        random_seed=seed,\n        loss_function='MAE',\n        verbose=False\n    )\n    model.fit(X_train, y_train)\n    cat_models.append(model)\n    print(f\"  CAT seed {seed} trained\")\n\nprint(f\"\\nTotal models: {len(xgb_models)} XGB, {len(lgb_models)} LGB, {len(cat_models)} CAT\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRODUCTION MODE] Training 5 models per algorithm\n",
      "\n",
      "Training XGBoost models...\n",
      "  XGB seed 42 trained\n",
      "  XGB seed 123 trained\n",
      "  XGB seed 456 trained\n",
      "  XGB seed 789 trained\n",
      "  XGB seed 2024 trained\n",
      "\n",
      "Training LightGBM models...\n",
      "  LGB seed 42 trained\n",
      "  LGB seed 123 trained\n",
      "  LGB seed 456 trained\n",
      "  LGB seed 789 trained\n",
      "  LGB seed 2024 trained\n",
      "\n",
      "Training CatBoost models...\n",
      "  CAT seed 42 trained\n",
      "  CAT seed 123 trained\n",
      "  CAT seed 456 trained\n",
      "  CAT seed 789 trained\n",
      "  CAT seed 2024 trained\n",
      "\n",
      "Total models: 5 XGB, 5 LGB, 5 CAT\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 4: Optimize Ensemble Weights (Including EWGM)\n\nFind the best weighted combination of all 4 model types using validation set"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:41:31.338662Z",
     "start_time": "2025-10-07T09:41:31.037326Z"
    }
   },
   "source": "# Get predictions from each model type (average across seeds)\nxgb_val_preds = np.mean([m.predict(X_val) for m in xgb_models], axis=0)\nlgb_val_preds = np.mean([m.predict(X_val) for m in lgb_models], axis=0)\ncat_val_preds = np.mean([m.predict(X_val) for m in cat_models], axis=0)\n\n# Generate EWGM predictions for validation set\nprint(\"Generating EWGM validation predictions...\")\n# Need to use only training data for EWGM (not validation)\newgm_val_preds = generate_ewgm_predictions(\n    train_data=new_house,  # Original training data\n    test_data=val_data[['month', 'sector']].assign(id=lambda x: x['month'] + '_' + x['sector']),\n    n_lags=6,\n    alpha=0.9\n)\nprint(f\"EWGM predictions: min={ewgm_val_preds.min():.2f}, max={ewgm_val_preds.max():.2f}, mean={ewgm_val_preds.mean():.2f}\")\n\nprint(\"\\nSearching for optimal ensemble weights (including EWGM)...\")\n\n# Grid search over weight combinations\n# w_xgb + w_lgb + w_cat + w_ewgm = 1.0\nif TEST_MODE:\n    grid = np.linspace(0, 1, 6)  # Even coarser grid with 4 models\nelse:\n    grid = np.linspace(0, 1, 11)  # Finer grid in production\n\nbest_score = -1\nbest_weights = None\n\nfor w_xgb in grid:\n    for w_lgb in grid:\n        for w_cat in grid:\n            w_ewgm = 1.0 - w_xgb - w_lgb - w_cat\n            \n            # Skip invalid weight combinations\n            if w_ewgm < 0 or w_ewgm > 1:\n                continue\n            \n            # Weighted ensemble\n            ensemble_pred = (w_xgb * xgb_val_preds + \n                           w_lgb * lgb_val_preds + \n                           w_cat * cat_val_preds + \n                           w_ewgm * ewgm_val_preds)\n            score = competition_metric(y_val, ensemble_pred, verbose=False)\n            \n            if score > best_score:\n                best_score = score\n                best_weights = (w_xgb, w_lgb, w_cat, w_ewgm)\n                print(f\"  New best: XGB={w_xgb:.2f}, LGB={w_lgb:.2f}, CAT={w_cat:.2f}, EWGM={w_ewgm:.2f}, score={score:.4f}\")\n\nprint(f\"\\n=== Best Ensemble Weights ===\")\nprint(f\"XGBoost:  {best_weights[0]:.3f}\")\nprint(f\"LightGBM: {best_weights[1]:.3f}\")\nprint(f\"CatBoost: {best_weights[2]:.3f}\")\nprint(f\"EWGM:     {best_weights[3]:.3f}\")\nprint(f\"Val Score: {best_score:.4f}\")\n\n# Validate the final ensemble\nfinal_val_pred = (\n    best_weights[0] * xgb_val_preds + \n    best_weights[1] * lgb_val_preds + \n    best_weights[2] * cat_val_preds +\n    best_weights[3] * ewgm_val_preds\n)\n\nprint(\"\\n=== Validation Performance ===\")\ncompetition_metric(y_val, final_val_pred)\nprint(f\"MAE: {mean_absolute_error(y_val, final_val_pred):.2f}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating EWGM validation predictions...\n",
      "EWGM predictions: min=0.00, max=117040.84, mean=21391.46\n",
      "\n",
      "Searching for optimal ensemble weights (including EWGM)...\n",
      "  New best: XGB=0.00, LGB=0.00, CAT=0.00, EWGM=1.00, score=0.4933\n",
      "  New best: XGB=0.00, LGB=0.00, CAT=0.10, EWGM=0.90, score=0.5299\n",
      "  New best: XGB=0.00, LGB=0.00, CAT=0.20, EWGM=0.80, score=0.5385\n",
      "  New best: XGB=0.00, LGB=0.00, CAT=0.30, EWGM=0.70, score=0.5487\n",
      "  New best: XGB=0.00, LGB=0.10, CAT=0.20, EWGM=0.70, score=0.5513\n",
      "\n",
      "=== Best Ensemble Weights ===\n",
      "XGBoost:  0.000\n",
      "LightGBM: 0.100\n",
      "CatBoost: 0.200\n",
      "EWGM:     0.700\n",
      "Val Score: 0.5513\n",
      "\n",
      "=== Validation Performance ===\n",
      "  APE > 100%: 25.9%\n",
      "  Valid samples: 74.1%\n",
      "  MAPE (valid): 0.3325\n",
      "  Scaled MAPE: 0.4487\n",
      "MAE: 8386.62\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 5: Retrain on ALL Data\n\nNow that we have best params and weights, retrain everything on full dataset for final submission"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:44:39.491940Z",
     "start_time": "2025-10-07T09:41:31.352736Z"
    }
   },
   "source": [
    "print(\"=== RETRAINING ON FULL DATA ===\")\n",
    "\n",
    "# Changed: rebuild full dense grid and features\n",
    "months = sorted(set(new_house['month']) | set(pre_owned['month']) | set(land_trans['month']))\n",
    "sectors = sorted(set(new_house['sector']) | set(pre_owned['sector']) | set(land_trans['sector']))\n",
    "full_grid = pd.DataFrame([(m, s) for m in months for s in sectors], columns=['month', 'sector'])\n",
    "full_grid = full_grid.merge(new_house[['month', 'sector', target]], on=['month', 'sector'], how='left')\n",
    "full_grid[target] = full_grid[target].fillna(0)\n",
    "\n",
    "full_data = create_base_features(full_grid, target=target)\n",
    "full_data = create_lag_features(full_data, target=target)\n",
    "full_data = add_sector_stats(full_data, full_data, target=target)\n",
    "\n",
    "X_full = full_data[feature_cols].fillna(0)\n",
    "y_full = full_data[target]\n",
    "\n",
    "print(f\"Full training set: {X_full.shape}\")\n",
    "\n",
    "# Calculate clipping bounds from full data\n",
    "hist_q99 = y_full.quantile(0.99)\n",
    "print(f\"Target range: {y_full.min():.2f} to {y_full.max():.2f}\")\n",
    "print(f\"Q99: {hist_q99:.2f}\")\n",
    "\n",
    "# Changed: per-sector caps from recent history (last 6 months), fallback to sector max\n",
    "last_train_month = full_data['month_date'].max()\n",
    "window_start = last_train_month - pd.DateOffset(months=6)\n",
    "recent = full_data[full_data['month_date'] >= window_start]\n",
    "sector_recent_max = recent.groupby('sector')[target].max()\n",
    "sector_all_max = full_data.groupby('sector')[target].max()\n",
    "sector_cap_series = (1.2 * sector_recent_max).fillna(1.2 * sector_all_max)\n",
    "sector_caps = sector_cap_series.to_dict()\n",
    "\n",
    "# Global fallback cap\n",
    "global_cap = float(hist_q99 * 1.5)\n",
    "print(f\"Will cap predictions per sector using recent history; global cap {global_cap:.2f}\")\n",
    "\n",
    "# Train final XGBoost models\n",
    "print(\"\\nTraining final XGBoost models...\")\n",
    "final_xgb_models = []\n",
    "for seed in seeds:\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=best_xgb_params['n_estimators'],\n",
    "        max_depth=best_xgb_params['max_depth'],\n",
    "        learning_rate=best_xgb_params['learning_rate'],\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        gamma=0.1,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=seed,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_full, y_full, verbose=False)\n",
    "    final_xgb_models.append(model)\n",
    "    print(f\"  XGB seed {seed} ✓\")\n",
    "\n",
    "# Train final LightGBM models\n",
    "print(\"\\nTraining final LightGBM models...\")\n",
    "final_lgb_models = []\n",
    "for seed in seeds:\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=best_lgb_params['n_estimators'],\n",
    "        max_depth=best_lgb_params['max_depth'],\n",
    "        learning_rate=best_lgb_params['learning_rate'],\n",
    "        num_leaves=2**best_lgb_params['max_depth'],\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=seed,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model.fit(X_full, y_full)\n",
    "    final_lgb_models.append(model)\n",
    "    print(f\"  LGB seed {seed} ✓\")\n",
    "\n",
    "# Train final CatBoost models\n",
    "print(\"\\nTraining final CatBoost models...\")\n",
    "final_cat_models = []\n",
    "for seed in seeds:\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=best_cat_params['iterations'],\n",
    "        depth=best_cat_params['depth'],\n",
    "        learning_rate=best_cat_params['learning_rate'],\n",
    "        l2_leaf_reg=3.0,\n",
    "        random_seed=seed,\n",
    "        loss_function='MAE',\n",
    "        verbose=False\n",
    "    )\n",
    "    model.fit(X_full, y_full)\n",
    "    final_cat_models.append(model)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RETRAINING ON FULL DATA ===\n",
      "Full training set: (7584, 264)\n",
      "Target range: 0.00 to 606407.64\n",
      "Q99: 224087.58\n",
      "Will cap predictions per sector using recent history; global cap 336131.37\n",
      "\n",
      "Training final XGBoost models...\n",
      "  XGB seed 42 ✓\n",
      "  XGB seed 123 ✓\n",
      "  XGB seed 456 ✓\n",
      "  XGB seed 789 ✓\n",
      "  XGB seed 2024 ✓\n",
      "\n",
      "Training final LightGBM models...\n",
      "  LGB seed 42 ✓\n",
      "  LGB seed 123 ✓\n",
      "  LGB seed 456 ✓\n",
      "  LGB seed 789 ✓\n",
      "  LGB seed 2024 ✓\n",
      "\n",
      "Training final CatBoost models...\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## STEP 6: Prepare Test Data\n\nApply same feature engineering to test set"
  },
  {
   "cell_type": "code",
   "source": "# Base features for test\ntest_data = create_base_features(test, target=target)\n\n# For lag features, need to concatenate with training data\n# so test can use recent history from training\ntest_combined = pd.concat([\n    full_data[['month', 'sector', 'month_date', target]],\n    test_data[['month', 'sector', 'month_date']].assign(**{target: np.nan})\n]).sort_values(['sector', 'month_date']).reset_index(drop=True)\n\n# Create lag features on combined data\ntest_combined = create_lag_features(test_combined, target=target)\n\n# Extract just the test portion\ntest_features = test_combined[test_combined[target].isna()].reset_index(drop=True)\n\n# Merge lag features back to test_data\nlag_cols = ['sector_target_encoded'] + \\\n           [f'amount_lag_{lag}' for lag in [1,2,3,4,6,12]] + \\\n           [f'amount_ewm_{span}' for span in [3,6,12]] + \\\n           [f'amount_rolling_mean_{w}' for w in [3,6,12]] + \\\n           [f'amount_rolling_std_{w}' for w in [3,6,12]] + \\\n           ['amount_lag_12_growth']\n\nfor col in lag_cols:\n    test_data[col] = test_features[col].values\n\n# Add sector stats from full training data\ntest_data = add_sector_stats(test_data, full_data, target=target)\n\n# Create feature matrix\nX_test = test_data[feature_cols].fillna(0)\n\nprint(f\"Test data: {X_test.shape}\")\nprint(f\"Features match: {list(X_full.columns) == list(X_test.columns)}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:44:39.981503Z",
     "start_time": "2025-10-07T09:44:39.741195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data: (1152, 264)\n",
      "Features match: True\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:44:40.390552Z",
     "start_time": "2025-10-07T09:44:39.988282Z"
    }
   },
   "source": [
    "# Average predictions from each model type\n",
    "xgb_test_preds = np.mean([m.predict(X_test) for m in final_xgb_models], axis=0)\n",
    "lgb_test_preds = np.mean([m.predict(X_test) for m in final_lgb_models], axis=0)\n",
    "cat_test_preds = np.mean([m.predict(X_test) for m in final_cat_models], axis=0)\n",
    "\n",
    "# Generate EWGM predictions for test set\n",
    "print(\"Generating EWGM test predictions...\")\n",
    "ewgm_test_preds = generate_ewgm_predictions(\n",
    "    train_data=new_house,  # Use full training data\n",
    "    test_data=test,\n",
    "    n_lags=6,\n",
    "    alpha=0.9\n",
    ")\n",
    "print(f\"EWGM predictions: min={ewgm_test_preds.min():.2f}, max={ewgm_test_preds.max():.2f}, mean={ewgm_test_preds.mean():.2f}\")\n",
    "\n",
    "# Weighted ensemble with optimized weights (including EWGM)\n",
    "test_pred = (\n",
    "    best_weights[0] * xgb_test_preds + \n",
    "    best_weights[1] * lgb_test_preds + \n",
    "    best_weights[2] * cat_test_preds +\n",
    "    best_weights[3] * ewgm_test_preds\n",
    ")\n",
    "\n",
    "# Changed: non-negativity and sector-wise clipping\n",
    "# Non-negativity first\n",
    "test_pred = np.maximum(test_pred, 0)\n",
    "\n",
    "# Sector-wise caps (fallback to global)\n",
    "caps = np.array([sector_caps.get(s, global_cap) for s in test['sector']])\n",
    "test_pred = np.minimum(test_pred, caps)\n",
    "\n",
    "# Changed: zero-history heuristic — if recent lags are all zero/missing, predict zero\n",
    "lag_check_cols = [f'amount_lag_{k}' for k in [1,2,3,6]]\n",
    "zero_hist_mask = test_data[lag_check_cols].fillna(0).sum(axis=1) == 0\n",
    "test_pred[zero_hist_mask.values] = 0\n",
    "\n",
    "# Zero out sectors that weren't in training data\n",
    "train_sectors = set(full_data['sector'].unique())\n",
    "unseen_mask = ~test['sector'].isin(train_sectors)\n",
    "test_pred[unseen_mask] = 0\n",
    "\n",
    "unseen_sectors = sorted([s for s in test['sector'].unique() if s not in train_sectors])\n",
    "print(f\"\\nUnseen sectors (set to 0): {unseen_sectors}\")\n",
    "print(f\"\\nFinal Predictions:\")\n",
    "print(f\"  Range: {test_pred.min():.2f} to {test_pred.max():.2f}\")\n",
    "print(f\"  Mean: {test_pred.mean():.2f}\")\n",
    "print(f\"  Median: {np.median(test_pred):.2f}\")\n",
    "print(f\"  Zeros: {(test_pred == 0).sum()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating EWGM test predictions...\n",
      "EWGM predictions: min=0.00, max=167256.95, mean=21926.91\n",
      "\n",
      "Unseen sectors (set to 0): []\n",
      "\n",
      "Final Predictions:\n",
      "  Range: 0.00 to 151855.81\n",
      "  Mean: 8566.48\n",
      "  Median: 0.00\n",
      "  Zeros: 684\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: Generate Predictions\n",
    "\n",
    "Use final models and optimal weights to create submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 8: Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T09:44:40.408878Z",
     "start_time": "2025-10-07T09:44:40.396799Z"
    }
   },
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'new_house_transaction_amount': test_pred\n",
    "})\n",
    "\n",
    "# Verify submission format\n",
    "orig_test = pd.read_csv(file_path + 'test.csv')\n",
    "assert submission['id'].tolist() == orig_test['id'].tolist(), \"ID order mismatch!\"\n",
    "assert np.isfinite(submission['new_house_transaction_amount']).all(), \"Non-finite predictions!\"\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"✅ Submission saved to submission.csv\")\n",
    "\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(\"\\nLast 10 predictions:\")\n",
    "print(submission.tail(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mode: {'TEST' if TEST_MODE else 'PRODUCTION'}\")\n",
    "print(f\"Best XGBoost params: {best_xgb_params}\")\n",
    "print(f\"Best LightGBM params: {best_lgb_params}\")\n",
    "print(f\"Best CatBoost params: {best_cat_params}\")\n",
    "print(f\"Ensemble weights: XGB={best_weights[0]:.3f}, LGB={best_weights[1]:.3f}, CAT={best_weights[2]:.3f}\")\n",
    "print(f\"Validation score: {best_score:.4f}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission saved to submission.csv\n",
      "\n",
      "First 10 predictions:\n",
      "                   id  new_house_transaction_amount\n",
      "0   2024 Aug_sector 1                   7328.270841\n",
      "1   2024 Aug_sector 2                   5248.022657\n",
      "2   2024 Aug_sector 3                   5585.699510\n",
      "3   2024 Aug_sector 4                  51257.223070\n",
      "4   2024 Aug_sector 5                   2930.945051\n",
      "5   2024 Aug_sector 6                  12990.197647\n",
      "6   2024 Aug_sector 7                      0.000000\n",
      "7   2024 Aug_sector 8                      0.000000\n",
      "8   2024 Aug_sector 9                      0.000000\n",
      "9  2024 Aug_sector 10                      0.000000\n",
      "\n",
      "Last 10 predictions:\n",
      "                      id  new_house_transaction_amount\n",
      "1142  2025 Jul_sector 87                      0.000000\n",
      "1143  2025 Jul_sector 88                      0.000000\n",
      "1144  2025 Jul_sector 89                      0.000000\n",
      "1145  2025 Jul_sector 90                   5160.152581\n",
      "1146  2025 Jul_sector 91                      0.000000\n",
      "1147  2025 Jul_sector 92                      0.000000\n",
      "1148  2025 Jul_sector 93                      0.000000\n",
      "1149  2025 Jul_sector 94                      0.000000\n",
      "1150  2025 Jul_sector 95                      0.000000\n",
      "1151  2025 Jul_sector 96                      0.000000\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Mode: PRODUCTION\n",
      "Best XGBoost params: {'n_estimators': 1500, 'max_depth': 7, 'learning_rate': 0.01}\n",
      "Best LightGBM params: {'n_estimators': 1500, 'max_depth': 7, 'learning_rate': 0.01}\n",
      "Best CatBoost params: {'iterations': 2000, 'depth': 7, 'learning_rate': 0.02}\n",
      "Ensemble weights: XGB=0.000, LGB=0.100, CAT=0.200\n",
      "Validation score: 0.5513\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
